# Complete ML-Agents Configuration Example
# This file demonstrates all available parameters with sensible defaults

behaviors:
  # Replace with your behavior name (e.g., "MoonlanderAgent", "3DBallAgent")
  YourAgentName:
    trainer_type: ppo
    
    # PPO Hyperparameters
    hyperparameters:
      batch_size: 256          # Mini-batch size for SGD
      buffer_size: 20480       # Experience buffer size (rollout buffer)
      learning_rate: 0.0003    # Initial learning rate
      learning_rate_schedule: linear  # 'linear' or 'constant'
      beta: 0.01               # Entropy coefficient (encourages exploration)
      epsilon: 0.2             # PPO clip parameter
      lambd: 0.95              # GAE lambda (bias-variance tradeoff)
      num_epoch: 3             # Number of epochs per update
    
    # Training Control
    max_steps: 500000          # Total training steps (not episodes!)
    time_horizon: 64           # Steps per agent before update
    summary_freq: 10000        # Log stats every N steps
    checkpoint_interval: 50000 # Save checkpoint every N steps
    keep_checkpoints: 10       # Number of checkpoints to keep
    
    # Network Architecture
    network_settings:
      normalize: true          # Normalize observations
      hidden_units: 128        # Neurons per hidden layer
      num_layers: 2            # Number of hidden layers
      # Optional: LSTM memory for partial observability
      # memory:
      #   sequence_length: 64
      #   memory_size: 128
    
    # Reward Signals
    reward_signals:
      extrinsic:
        gamma: 0.99            # Discount factor
        strength: 1.0          # Reward scale
        # Optional: separate network for value function
        # network_settings:
        #   normalize: true
        #   hidden_units: 128
        #   num_layers: 2
    
    # Optional: Load existing model
    # init_path: results/run_id/YourAgentName/checkpoint.pt

# Environment Settings
env_settings:
  # Path to Unity executable (or null for Editor mode)
  env_path: "/path/to/your/unity/build.app"  # macOS
  # env_path: "/path/to/your/unity/build.x86_64"  # Linux
  # env_path: "C:/path/to/your/unity/build.exe"   # Windows
  
  # Network Configuration
  base_port: 5005            # Starting port for communication
  num_envs: 1                # Number of parallel environments (1 for now)
  num_areas: 1               # Parallel training areas per environment
  timeout_wait: 60           # Seconds to wait for Unity connection
  
  # Process Management
  seed: -1                   # Random seed (-1 = random)
  max_lifetime_restarts: 10  # Max crashes allowed
  restarts_rate_limit_n: 1   # Max restarts in period
  restarts_rate_limit_period_s: 60  # Period in seconds
  
  # Optional: Environment parameters (passed to Unity)
  # These can be accessed in Unity via Academy.Instance.EnvironmentParameters
  # environment_parameters:
  #   difficulty: 1.0
  #   gravity_multiplier: 1.0

# Unity Engine Settings (performance optimization)
engine_settings:
  width: 84                  # Window width (pixels)
  height: 84                 # Window height (pixels)
  quality_level: 5           # Unity quality setting (0-5)
  time_scale: 20.0           # Simulation speed (higher = faster)
  target_frame_rate: -1      # FPS limit (-1 = unlimited)
  capture_frame_rate: 60     # Capture FPS (for video)
  no_graphics: true          # Disable rendering for performance
  no_graphics_monitor: false # Main worker with graphics, others without

# Environment-Specific Parameters (Custom)
# These are sent via Side Channel and accessible in Unity
environment_parameters:
  # Example parameters - customize for your environment
  # Unity access: Academy.Instance.EnvironmentParameters.GetWithDefault("key", default)
  lesson: 0                  # Curriculum learning lesson
  difficulty: 1.0            # Difficulty level
  # Add your custom parameters here

# Checkpoint Settings
checkpoint_settings:
  run_id: ppo                # Unique run identifier
  initialize_from: null      # Load weights from another run_id
  load_model: false          # Legacy (use init_path instead)
  resume: false              # Resume training from checkpoint
  force: false               # Overwrite existing run
  train_model: true          # Enable training (false = inference only)
  inference: false           # Inference-only mode
  results_dir: results       # Base directory for outputs

# Backend Settings
torch_settings:
  device: cpu                # 'cpu', 'cuda', or 'mps' (Apple Silicon)
  # Note: Current Rust implementation uses Burn with NdArray backend (CPU only)
  # For GPU training, consider using wgpu backend in future versions

# Debug Mode (verbose logging)
debug: false

# ============================================================================
# PERFORMANCE TUNING GUIDE
# ============================================================================

# For FASTER training:
# - Increase time_scale (50-100)
# - Enable no_graphics: true
# - Reduce quality_level (0-1)
# - Increase num_envs (when multi-env is ready)
# - Increase batch_size and buffer_size
# - Reduce target_frame_rate or set to -1

# For BETTER results:
# - Increase max_steps (500k-10M)
# - Tune learning_rate (try 3e-4 to 3e-5)
# - Adjust entropy beta (0.001-0.01)
# - Increase hidden_units and num_layers
# - Add LSTM memory for partial observability
# - Tune time_horizon based on episode length

# For CURRICULUM learning:
# - Start with easy environment_parameters
# - Monitor performance via summary_freq
# - Manually increase difficulty in YAML
# - Or implement auto-curriculum (future feature)

# ============================================================================
# COMMON CONFIGURATIONS
# ============================================================================

# Simple continuous control (e.g., CartPole):
# - hidden_units: 64
# - num_layers: 2
# - time_horizon: 64
# - buffer_size: 2048

# Complex continuous control (e.g., Humanoid):
# - hidden_units: 512
# - num_layers: 3
# - time_horizon: 4000
# - buffer_size: 20480
# - Add LSTM memory

# Discrete actions (e.g., GridWorld):
# - Same as continuous but adjust epsilon (0.1-0.2)
# - Consider increasing entropy beta

# Visual observations:
# - Use CNN architecture (not yet implemented)
# - Increase batch_size and buffer_size
# - Normalize: true is critical

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

# Unity not connecting:
# 1. Check env_path is correct
# 2. Verify base_port is available (lsof -i :5005)
# 3. Check Unity logs for errors
# 4. Try running Unity Editor in Play mode first
# 5. Increase timeout_wait if slow startup

# Training unstable:
# 1. Reduce learning_rate (try 1e-4)
# 2. Increase batch_size
# 3. Adjust epsilon clipping (0.1-0.3)
# 4. Check reward scaling (use reward_signals.extrinsic.strength)
# 5. Enable normalize: true

# Slow training:
# 1. Increase time_scale
# 2. Enable no_graphics
# 3. Reduce quality_level
# 4. Use multiple environments (when ready)
# 5. Check CPU usage (Burn is CPU-bound currently)

# Out of memory:
# 1. Reduce buffer_size
# 2. Reduce batch_size
# 3. Reduce num_envs
# 4. Reduce hidden_units or num_layers
